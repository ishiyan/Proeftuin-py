{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From c:\\Users\\ivas\\AppData\\Local\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
            "\n"
          ]
        },
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'rockrl'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[1], line 17\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfinrock\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DifferentActions, AccountValue, MaxDrawdown, SharpeRatio\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfinrock\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mindicators\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BolingerBands, RSI, PSAR, SMA, MACD\n\u001b[1;32m---> 17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mrockrl\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmisc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MeanAverage\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mrockrl\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmemory\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MemoryManager\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mrockrl\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PPOAgent\n",
            "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'rockrl'"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "tf.get_logger().setLevel('ERROR')\n",
        "for gpu in tf.config.experimental.list_physical_devices('GPU'):\n",
        "    tf.config.experimental.set_memory_growth(gpu, True)\n",
        "\n",
        "from keras import layers, models\n",
        "\n",
        "from finrock.data_feeder import PdDataFeeder\n",
        "from finrock.trading_env import TradingEnv\n",
        "from finrock.scalers import MinMaxScaler, ZScoreScaler\n",
        "from finrock.reward import SimpleReward, AccountValueChangeReward\n",
        "from finrock.metrics import DifferentActions, AccountValue, MaxDrawdown, SharpeRatio\n",
        "from finrock.indicators import BolingerBands, RSI, PSAR, SMA, MACD\n",
        "\n",
        "from rockrl.utils.misc import MeanAverage\n",
        "from rockrl.utils.memory import MemoryManager\n",
        "from rockrl.tensorflow import PPOAgent\n",
        "from rockrl.utils.vectorizedEnv import VectorizedEnv\n",
        "\n",
        "# The following allows to save plots in SVG format.\n",
        "import matplotlib_inline\n",
        "%matplotlib inline\n",
        "matplotlib_inline.backend_inline.set_matplotlib_formats('svg')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "pygame 2.5.2 (SDL 2.28.3, Python 3.11.5)\n",
            "Hello from the pygame community. https://www.pygame.org/contribute.html\n",
            "1007.7134780389868 0.6123856339139063\n",
            "1090.5607599465382 0.5834140165388254\n",
            "846.6079315479225 0.3566458368008232\n",
            "365.1254298103569 -0.37882083613511625\n",
            "1915.0183999296175 1.2448783426793206\n",
            "1354.2931805116207 0.15978317389131486\n",
            "733.5148192828475 0.2843296256162624\n",
            "1128.2484363607632 0.7170146311645123\n",
            "2419.7279037625744 0.5996566291942848\n",
            "2322.4871577574663 0.6823150472780253\n",
            "864.3927569584265 0.42316124987873255\n",
            "236.87971702222544 -0.9249763557178783\n",
            "681.5403164544242 0.1297951333098099\n",
            "1089.7544202141632 0.531591718689451\n",
            "595.1160152777511 -0.005048374172072281\n",
            "2065.4706098426273 0.31158160419666364\n",
            "6874.459650816223 1.1661137526697425\n",
            "1146.4553799780242 0.5980774592489487\n",
            "1918.9167765851373 -0.11130718481958958\n",
            "2210.257708239037 0.7926537560861525\n",
            "651.3874451898511 -0.47676957364820255\n",
            "1788.3558763363394 0.4295761584173775\n",
            "2050.430960591166 1.2678191173795266\n",
            "4400.8434157897755 0.7891503132368372\n",
            "2368.6084628652743 0.8066619532099196\n",
            "741.0907709343097 0.1949808476182231\n",
            "765.9828461298848 0.03665561724245132\n",
            "768.634853467124 -0.1777271652008625\n",
            "1489.3228285529954 0.8614824739156045\n",
            "1180.069100274343 0.7814355567474288\n",
            "708.252261993434 0.2085858695746163\n",
            "2929.62554210733 0.967588886657885\n",
            "782.226208073848 -0.3220786556852539\n",
            "740.3526828314131 0.3477374358954991\n",
            "659.7866234857489 0.12561883082131614\n",
            "1421.796031416491 0.3750343675034331\n",
            "2391.980978446207 1.5308680977831897\n",
            "700.029408054556 0.12871622627402618\n",
            "514.7213518313687 -0.18895579272927301\n",
            "3817.8297134412524 1.8450480639378508\n",
            "2674.9281044650475 0.2346321205774649\n",
            "4706.26758834206 0.3084632117692724\n",
            "2795.857465343937 0.6724476812791929\n",
            "1854.2877540767226 -0.1525443595905041\n",
            "3617.287090762173 1.945171073830651\n",
            "856.1843418536411 0.43055686418156847\n",
            "689.8116128531533 0.10309127182980031\n",
            "535.971525544624 -0.08540687252859529\n",
            "1240.3279456749276 0.7991159017083617\n",
            "883.9505302627633 0.46132135208321695\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[2], line 42\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m     39\u001b[0m     \u001b[38;5;66;03m# simulate model prediction, now use random action\u001b[39;00m\n\u001b[0;32m     40\u001b[0m     action \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;241m0\u001b[39m, action_space)\n\u001b[1;32m---> 42\u001b[0m     state, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[0;32m     43\u001b[0m     rewards \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward\n\u001b[0;32m     44\u001b[0m     pygameRender\u001b[38;5;241m.\u001b[39mrender(info)\n",
            "File \u001b[1;32md:\\Mbrane\\Repos\\Proeftuin-py\\pythonlessons-FinRock\\finrock\\trading_env.py:149\u001b[0m, in \u001b[0;36mTradingEnv.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    143\u001b[0m truncated \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_env_step_indexes \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    144\u001b[0m info \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    145\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstates\u001b[39m\u001b[38;5;124m\"\u001b[39m: [observation],\n\u001b[0;32m    146\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetrics\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_metricsHandler(observation)\n\u001b[0;32m    147\u001b[0m     }\n\u001b[1;32m--> 149\u001b[0m transformed_obs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_transformer\u001b[38;5;241m.\u001b[39mtransform(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_observations)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39misnan(transformed_obs)\u001b[38;5;241m.\u001b[39many():\n\u001b[0;32m    152\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransformed_obs contains nan values, check your data\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "File \u001b[1;32md:\\Mbrane\\Repos\\Proeftuin-py\\pythonlessons-FinRock\\finrock\\scalers.py:74\u001b[0m, in \u001b[0;36mZScoreScaler.transform\u001b[1;34m(self, observations)\u001b[0m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;66;03m# nan to zero, when divided by zero and allocation_percentage is not changed\u001b[39;00m\n\u001b[0;32m     72\u001b[0m returns \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mnan_to_num(np\u001b[38;5;241m.\u001b[39mdiff(results, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m/\u001b[39m results[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m---> 74\u001b[0m z_scores \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mnan_to_num((returns \u001b[38;5;241m-\u001b[39m np\u001b[38;5;241m.\u001b[39mmean(returns, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)) \u001b[38;5;241m/\u001b[39m np\u001b[38;5;241m.\u001b[39mstd(returns, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m))\n\u001b[0;32m     76\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m z_scores\n",
            "File \u001b[1;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36mmean\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
            "File \u001b[1;32mc:\\Users\\Binck\\anaconda3\\Lib\\site-packages\\numpy\\core\\fromnumeric.py:3464\u001b[0m, in \u001b[0;36mmean\u001b[1;34m(a, axis, dtype, out, keepdims, where)\u001b[0m\n\u001b[0;32m   3461\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   3462\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m mean(axis\u001b[38;5;241m=\u001b[39maxis, dtype\u001b[38;5;241m=\u001b[39mdtype, out\u001b[38;5;241m=\u001b[39mout, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m-> 3464\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _methods\u001b[38;5;241m.\u001b[39m_mean(a, axis\u001b[38;5;241m=\u001b[39maxis, dtype\u001b[38;5;241m=\u001b[39mdtype,\n\u001b[0;32m   3465\u001b[0m                       out\u001b[38;5;241m=\u001b[39mout, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[1;32mc:\\Users\\Binck\\anaconda3\\Lib\\site-packages\\numpy\\core\\_methods.py:181\u001b[0m, in \u001b[0;36m_mean\u001b[1;34m(a, axis, dtype, out, keepdims, where)\u001b[0m\n\u001b[0;32m    178\u001b[0m         dtype \u001b[38;5;241m=\u001b[39m mu\u001b[38;5;241m.\u001b[39mdtype(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mf4\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    179\u001b[0m         is_float16_result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m--> 181\u001b[0m ret \u001b[38;5;241m=\u001b[39m umr_sum(arr, axis, dtype, out, keepdims, where\u001b[38;5;241m=\u001b[39mwhere)\n\u001b[0;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ret, mu\u001b[38;5;241m.\u001b[39mndarray):\n\u001b[0;32m    183\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m _no_nep50_warning():\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "df = pd.read_csv('Datasets/random_sinusoid.csv')\n",
        "df = df[:-1000]\n",
        "\n",
        "\n",
        "pd_data_feeder = PdDataFeeder(\n",
        "    df,\n",
        "    indicators = [\n",
        "        BolingerBands(data=df, period=20, std=2),\n",
        "        RSI(data=df, period=14),\n",
        "        PSAR(data=df),\n",
        "        MACD(data=df),\n",
        "        SMA(data=df, period=7),\n",
        "    ]\n",
        ")\n",
        "\n",
        "num_envs = 10\n",
        "env = VectorizedEnv(\n",
        "    env_object = TradingEnv,\n",
        "    num_envs = num_envs,\n",
        "    data_feeder = pd_data_feeder,\n",
        "    output_transformer = ZScoreScaler(),\n",
        "    initial_balance = 1000.0,\n",
        "    max_episode_steps = 1000,\n",
        "    window_size = 50,\n",
        "    reward_function = AccountValueChangeReward(),\n",
        "    metrics = [\n",
        "        DifferentActions(),\n",
        "        AccountValue(),\n",
        "        MaxDrawdown(),\n",
        "        SharpeRatio(),\n",
        "    ]\n",
        ")\n",
        "\n",
        "action_space = env.action_space\n",
        "input_shape = env.observation_space.shape\n",
        "\n",
        "def actor_model(input_shape, action_space):\n",
        "    input = layers.Input(shape=input_shape, dtype=tf.float32)\n",
        "    x = layers.Flatten()(input)\n",
        "    x = layers.Dense(512, activation='elu')(x)\n",
        "    x = layers.Dense(256, activation='elu')(x)\n",
        "    x = layers.Dense(64, activation='elu')(x)\n",
        "    x = layers.Dropout(0.2)(x)\n",
        "    output = layers.Dense(action_space, activation='softmax')(x) # discrete action space\n",
        "    return models.Model(inputs=input, outputs=output)\n",
        "\n",
        "def critic_model(input_shape):\n",
        "    input = layers.Input(shape=input_shape, dtype=tf.float32)\n",
        "    x = layers.Flatten()(input)\n",
        "    x = layers.Dense(512, activation='elu')(x)\n",
        "    x = layers.Dense(256, activation='elu')(x)\n",
        "    x = layers.Dense(64, activation='elu')(x)\n",
        "    x = layers.Dropout(0.2)(x)\n",
        "    output = layers.Dense(1, activation=None)(x)\n",
        "    return models.Model(inputs=input, outputs=output)\n",
        "\n",
        "agent = PPOAgent(\n",
        "    actor = actor_model(input_shape, action_space),\n",
        "    critic = critic_model(input_shape),\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),\n",
        "    batch_size=128,\n",
        "    lamda=0.95,\n",
        "    kl_coeff=0.5,\n",
        "    c2=0.01,\n",
        "    writer_comment='ppo_sinusoid_discrete',\n",
        ")\n",
        "\n",
        "pd_data_feeder.save_config(agent.logdir)\n",
        "env.env.save_config(agent.logdir)\n",
        "\n",
        "memory = MemoryManager(num_envs=num_envs)\n",
        "meanAverage = MeanAverage(best_mean_score_episode=1000)\n",
        "states, infos = env.reset()\n",
        "rewards = 0.0\n",
        "while True:\n",
        "    action, prob = agent.act(states)\n",
        "\n",
        "    next_states, reward, terminated, truncated, infos = env.step(action)\n",
        "    memory.append(states, action, reward, prob, terminated, truncated, next_states, infos)\n",
        "    states = next_states\n",
        "\n",
        "    for index in memory.done_indices():\n",
        "        env_memory = memory[index]\n",
        "        history = agent.train(env_memory)\n",
        "        mean_reward = meanAverage(np.sum(env_memory.rewards))\n",
        "\n",
        "        if meanAverage.is_best(agent.epoch):\n",
        "            agent.save_models('ppo_sinusoid')\n",
        "\n",
        "        if history['kl_div'] > 0.05 and agent.epoch > 1000:\n",
        "            agent.reduce_learning_rate(0.995, verbose=False)\n",
        "\n",
        "        info = env_memory.infos[-1]\n",
        "        print(agent.epoch, np.sum(env_memory.rewards), mean_reward, info[\"metrics\"]['account_value'], history['kl_div'])\n",
        "        agent.log_to_writer(info['metrics'])\n",
        "        states[index], infos[index] = env.reset(index=index)\n",
        "\n",
        "    if agent.epoch >= 10000:\n",
        "        break\n",
        "\n",
        "env.close()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyOSYhPs1lX76ee7V5qmoI3J",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
