{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From c:\\Users\\ivas\\AppData\\Local\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "tf.get_logger().setLevel('ERROR')\n",
        "for gpu in tf.config.experimental.list_physical_devices('GPU'):\n",
        "    tf.config.experimental.set_memory_growth(gpu, True)\n",
        "\n",
        "from finrock.data_feeder import PdDataFeeder\n",
        "from finrock.trading_env import TradingEnv\n",
        "from finrock.render import PygameRender\n",
        "\n",
        "# The following allows to save plots in SVG format.\n",
        "import matplotlib_inline\n",
        "%matplotlib inline\n",
        "matplotlib_inline.backend_inline.set_matplotlib_formats('svg')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df = pd.read_csv('Datasets/random_sinusoid.csv')\n",
        "df = df[-1000:]\n",
        "\n",
        "model_path = \"runs/1704798174\" # CHANGE THIS !!!!\n",
        "\n",
        "pd_data_feeder = PdDataFeeder.load_config(df, model_path)\n",
        "env = TradingEnv.load_config(pd_data_feeder, model_path)\n",
        "\n",
        "action_space = env.action_space\n",
        "input_shape = env.observation_space.shape\n",
        "pygameRender = PygameRender(frame_rate=120)\n",
        "\n",
        "agent = tf.keras.models.load_model(f'{model_path}/ppo_sinusoid_actor.h5')\n",
        "\n",
        "state, info = env.reset()\n",
        "pygameRender.render(info)\n",
        "rewards = 0.0\n",
        "while True:\n",
        "    # simulate model prediction, now use random action\n",
        "    action = agent.predict(np.expand_dims(state, axis=0), verbose=False)[0][:-1]\n",
        "\n",
        "    state, reward, terminated, truncated, info = env.step(action)\n",
        "    rewards += reward\n",
        "    pygameRender.render(info)\n",
        "\n",
        "    if terminated or truncated:\n",
        "        print(rewards)\n",
        "        for metric, value in info['metrics'].items():\n",
        "            print(metric, value)\n",
        "        state, info = env.reset()\n",
        "        rewards = 0.0\n",
        "        pygameRender.reset()\n",
        "        pygameRender.render(info)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyOSYhPs1lX76ee7V5qmoI3J",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
